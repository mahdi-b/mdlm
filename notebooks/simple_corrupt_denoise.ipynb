{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple corruption and denoising demo\n",
    "This notebook shows how to corrupt a sentence, denoise it and train a small model using this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from diffusion import Diffusion\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "if tokenizer.mask_token is None:\n",
    "    tokenizer.add_special_tokens({'mask_token': '[MASK]'})\n",
    "mask_id = tokenizer.mask_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog.',\n",
    "    'A second example for training.',\n",
    "    'Another simple sentence for the dataset.',\n",
    "    'Learning diffusion models is fun.',\n",
    "    'Masked language modeling is interesting.'\n",
    "]\n",
    "enc = tokenizer(sentences, padding='max_length', truncation=True, max_length=16, return_tensors='pt')\n",
    "input_ids = enc['input_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corruptor:\n",
    "    def __init__(self, mask_index):\n",
    "        self.mask_index = mask_index\n",
    "    def corrupt(self, tokens, move_chance):\n",
    "        return Diffusion.q_xt(self, tokens, move_chance)\n",
    "\n",
    "corruptor = Corruptor(mask_id)\n",
    "noisy_input = corruptor.corrupt(input_ids, torch.tensor([[0.3]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallMLM(torch.nn.Module):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(vocab, 64)\n",
    "        self.rnn = torch.nn.LSTM(64, 64, batch_first=True)\n",
    "        self.out = torch.nn.Linear(64, vocab)\n",
    "    def forward(self, x):\n",
    "        e = self.embed(x)\n",
    "        h,_ = self.rnn(e)\n",
    "        return self.out(h)\n",
    "\n",
    "model = SmallMLM(tokenizer.vocab_size)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_noisy, x_clean):\n",
    "    logits = model(x_noisy)\n",
    "    labels = x_clean.clone()\n",
    "    labels[x_noisy != mask_id] = -100\n",
    "    loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    return loss.item()\n",
    "\n",
    "for epoch in range(3):\n",
    "    loss = train_step(noisy_input, input_ids)\n",
    "    print('epoch', epoch, 'loss', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise(tokens):\n",
    "    with torch.no_grad():\n",
    "        preds = model(tokens).argmax(-1)\n",
    "        return torch.where(tokens==mask_id, preds, tokens)\n",
    "\n",
    "denoised = denoise(noisy_input)\n",
    "for i in range(len(sentences)):\n",
    "    print('noisy:', tokenizer.decode(noisy_input[i]))\n",
    "    print('denoised:', tokenizer.decode(denoised[i]))\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
